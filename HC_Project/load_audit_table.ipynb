{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4b1b7e-2b32-43c2-a99c-c316880627a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import time\n",
    "\n",
    "audit_tbl = \"edl_hc_mart.audit.audit_ingestion\"\n",
    "run_start = F.current_timestamp()\n",
    "\n",
    "def audit_start(r, attempt=1, watermark_col=None, last_success_wm=None, checkpoint_path=None):\n",
    "    df = (spark.createDataFrame([{\n",
    "        \"pipeline_name\": r.pipeline_name,\n",
    "        \"source_type\": r.source_type,\n",
    "        \"source_name\": r.source_name,\n",
    "        \"bronze_table\": r.bronze_table,\n",
    "        \"batch_id\": r.batch_id,\n",
    "        \"run_id\": r.run_id,\n",
    "        \"trigger_type\": \"schedule\",\n",
    "        \"attempt\": attempt,\n",
    "        \"run_start_ts\": None,   # set below for deterministic equality\n",
    "        \"run_end_ts\": None,\n",
    "        \"duration_ms\": None,\n",
    "        \"last_status\": \"Started\",\n",
    "        \"records_read\": None,\n",
    "        \"records_written\": None,\n",
    "        \"error_count\": None,\n",
    "        \"error_message\": None,\n",
    "        \"watermark_col\": watermark_col,\n",
    "        \"last_success_watermark_value\": last_success_wm,\n",
    "        \"current_run_high_watermark_value\": None,\n",
    "        \"file_checkpoint_path\": checkpoint_path,\n",
    "        \"schema_version_applied\": r.schema_version,\n",
    "        \"producer_system\": r.producer_system,\n",
    "        \"ingestion_user\": r.ingestion_user,\n",
    "        \"notes\": r.notes,\n",
    "        \"last_load_date\": None,\n",
    "        \"created_at\": None,\n",
    "        \"updated_at\": None\n",
    "    }], schema=\"\"\"\n",
    "        pipeline_name string, source_type string, source_name string, bronze_table string,\n",
    "        batch_id string, run_id string, trigger_type string, attempt int,\n",
    "        run_start_ts timestamp, run_end_ts timestamp, duration_ms long,\n",
    "        last_status string, records_read long, records_written long, error_count long, error_message string,\n",
    "        watermark_col string, last_success_watermark_value string, current_run_high_watermark_value string,\n",
    "        file_checkpoint_path string, schema_version_applied string, producer_system string,\n",
    "        ingestion_user string, notes string, last_load_date date, created_at timestamp, updated_at timestamp\n",
    "    \"\"\")\n",
    "         .withColumn(\"run_start_ts\", F.current_timestamp())\n",
    "         .withColumn(\"created_at\", F.current_timestamp())\n",
    "         .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(audit_tbl)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db8dcce-6125-415e-8008-f524d29fde98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def audit_success(r, records_read, records_written, high_watermark_value):\n",
    "    # Append a success record (immutable log) for simplicity\n",
    "    df = (spark.createDataFrame([{\n",
    "        \"pipeline_name\": r.pipeline_name,\n",
    "        \"source_type\": r.source_type,\n",
    "        \"source_name\": r.source_name,\n",
    "        \"bronze_table\": r.bronze_table,\n",
    "        \"batch_id\": r.batch_id,\n",
    "        \"run_id\": r.run_id,\n",
    "        \"trigger_type\": \"schedule\",\n",
    "        \"attempt\": 1,\n",
    "        \"run_start_ts\": None,\n",
    "        \"run_end_ts\": None,\n",
    "        \"duration_ms\": None,\n",
    "        \"last_status\": \"Success\",\n",
    "        \"records_read\": records_read,\n",
    "        \"records_written\": records_written,\n",
    "        \"error_count\": 0,\n",
    "        \"error_message\": None,\n",
    "        \"watermark_col\": \"ingestion_ts\",  # or your chosen column\n",
    "        \"last_success_watermark_value\": high_watermark_value,\n",
    "        \"current_run_high_watermark_value\": high_watermark_value,\n",
    "        \"file_checkpoint_path\": None,\n",
    "        \"schema_version_applied\": r.schema_version,\n",
    "        \"producer_system\": r.producer_system,\n",
    "        \"ingestion_user\": r.ingestion_user,\n",
    "        \"notes\": r.notes,\n",
    "        \"last_load_date\": F.to_date(F.current_timestamp()),\n",
    "        \"created_at\": None,\n",
    "        \"updated_at\": None\n",
    "    }], schema=spark.table(audit_tbl).schema)\n",
    "         .withColumn(\"run_start_ts\", F.current_timestamp()) # optional; or omit\n",
    "         .withColumn(\"run_end_ts\", F.current_timestamp())\n",
    "         .withColumn(\"duration_ms\", F.lit(0))  # compute if you capture start/end\n",
    "         .withColumn(\"created_at\", F.current_timestamp())\n",
    "         .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    )\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(audit_tbl)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf8aeb0-58ec-47d5-981a-a0091b0f2421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def audit_failure(r, error_message, high_watermark_observed=None, attempt=1):\n",
    "    df = (spark.createDataFrame([{\n",
    "        \"pipeline_name\": r.pipeline_name,\n",
    "        \"source_type\": r.source_type,\n",
    "        \"source_name\": r.source_name,\n",
    "        \"bronze_table\": r.bronze_table,\n",
    "        \"batch_id\": r.batch_id,\n",
    "        \"run_id\": r.run_id,\n",
    "        \"trigger_type\": \"schedule\",\n",
    "        \"attempt\": attempt,\n",
    "        \"run_start_ts\": None,\n",
    "        \"run_end_ts\": None,\n",
    "        \"duration_ms\": None,\n",
    "        \"last_status\": \"Failure\",\n",
    "        \"records_read\": None,\n",
    "        \"records_written\": None,\n",
    "        \"error_count\": 1,\n",
    "        \"error_message\": error_message[:1000],  # truncate to avoid giant rows\n",
    "        \"watermark_col\": \"ingestion_ts\",\n",
    "        \"last_success_watermark_value\": None,   # not promoting\n",
    "        \"current_run_high_watermark_value\": high_watermark_observed,\n",
    "        \"file_checkpoint_path\": None,\n",
    "        \"schema_version_applied\": r.schema_version,\n",
    "        \"producer_system\": r.producer_system,\n",
    "        \"ingestion_user\": r.ingestion_user,\n",
    "        \"notes\": r.notes,\n",
    "        \"last_load_date\": F.to_date(F.current_timestamp()),\n",
    "        \"created_at\": None,\n",
    "        \"updated_at\": None\n",
    "    }], schema=spark.table(audit_tbl).schema)\n",
    "         .withColumn(\"run_start_ts\", F.current_timestamp())\n",
    "         .withColumn(\"run_end_ts\", F.current_timestamp())\n",
    "         .withColumn(\"duration_ms\", F.lit(0))\n",
    "         .withColumn(\"created_at\", F.current_timestamp())\n",
    "         .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    )\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(audit_tbl)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f72b826-fcd9-4c9a-894e-adcad74d42ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REPLACE VIEW edl_hc_mart.audit.vw_audit_ingestion_last_success AS\n",
    "WITH ranked AS (\n",
    "  SELECT\n",
    "    pipeline_name, source_name, bronze_table,\n",
    "    MAX(CASE WHEN last_status = 'Success' THEN created_at END) AS last_success_ts\n",
    "  FROM edl_hc_mart.audit.audit_ingestion\n",
    "  GROUP BY pipeline_name, source_name, bronze_table\n",
    ")\n",
    "SELECT a.pipeline_name, a.source_name, a.bronze_table,\n",
    "       a.watermark_col,\n",
    "       a.last_success_watermark_value\n",
    "FROM edl_hc_mart.audit.audit_ingestion a\n",
    "JOIN ranked r\n",
    "  ON a.pipeline_name = r.pipeline_name\n",
    " AND a.source_name   = r.source_name\n",
    " AND a.bronze_table  = r.bronze_table\n",
    " AND a.created_at    = r.last_success_ts\n",
    "WHERE a.last_status = 'Success';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7f3c33-7ad1-40f1-8c4a-ad29e13cfbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM edl_hc_mart.audit.vw_audit_ingestion_last_success;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6551436382344241,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "load_audit_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
