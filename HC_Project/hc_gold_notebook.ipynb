{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c11315-08c4-491d-bd49-9f2d9209c27c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold Employee"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, lit\n",
    "\n",
    "# Load Silver\n",
    "emp  = spark.sql(\"Select * from edl_hc_datamart.silver.employees WHERE is_active = True\")\n",
    "\n",
    "# DIM EMPLOYEE\n",
    "# Only select columns matching the existing Delta table schema\n",
    "# Remove 'employee_id' from the select statement\n",
    "\n",
    "dim_employee = (emp\n",
    "    .select(\n",
    "        col(\"employee_id\"),\n",
    "        \"first_name\",\"last_name\",\"email\",\"date_of_birth_fmt\",\"dept_code\"\n",
    "    ).filter(col(\"is_active\") == True).withColumnRenamed(\"employee_id\", \"employee_key\"))\n",
    "\n",
    "# dim_employee.display()\n",
    "dim_employee.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.gold.dim_employees\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e18ab2c-1ad4-4bc3-9f9d-129fec4ab859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold Department"
    }
   },
   "outputs": [],
   "source": [
    "# DIM DEPARTMENT\n",
    "# Only select columns matching the existing Delta table schema\n",
    "\n",
    "dept = spark.sql(\"Select * from edl_hc_datamart.silver.departments\")\n",
    "\n",
    "dim_department = dept.select(\n",
    "    col(\"dept_code\"),\n",
    "    \"dept_name\"\n",
    ").filter(col(\"is_active\") == True)\n",
    "\n",
    "dim_department.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.gold.dim_departments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbaa5ae6-26a5-461d-8d63-939e2795b64d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dim Date"
    }
   },
   "outputs": [],
   "source": [
    "# DIM Date\n",
    "# Only select columns matching the existing Delta table schema\n",
    "\n",
    "jobs = spark.sql(\"SELECT * from edl_hc_datamart.silver.jobs\")\n",
    "\n",
    "# DIM DATE (derived from job start dates)\n",
    "dim_date = (jobs\n",
    "    .select(\"start_date\").distinct()\n",
    "    .withColumnRenamed(\"start_date\",\"date_key\")\n",
    "    .withColumn(\"date_label\", date_format(col(\"date_key\"), \"dd-MMM-yyyy\")).filter(col(\"is_active\") == True))\n",
    "\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.gold.dim_date\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9563ab0a-02a6-4b90-a0ee-0920dc8e00d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fact containing Job History of employee"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# FACT JOB HISTORY\n",
    "fact_job_history = (jobs.alias(\"j\")\n",
    "    .join(emp.alias(\"e\"), [\"employee_id\"], \"left\")\n",
    "    .join(dept.alias(\"d\"), col(\"d.dept_code\")==col(\"e.dept_code\"), \"left\")\n",
    "    .select(\n",
    "        col(\"j.employee_id\").alias(\"employee_key\"),\n",
    "        col(\"j.position_id\"),\n",
    "        col(\"j.job_title\"),\n",
    "        col(\"j.start_date\").alias(\"start_date_key\"),\n",
    "        col(\"j.end_date\").alias(\"end_date_key\"),\n",
    "        col(\"j.status\"),\n",
    "        col(\"d.dept_code\").alias(\"dept_key\")\n",
    "    ).filter(col(\"e.is_active\")== True))\n",
    "df_fact = fact_job_history.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.gold.fact_job_history\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99c78c8-d706-44b6-ac51-22fb3de840cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW edl_hc_datamart.gold_views.employee_job_history_vw WITH SCHEMA EVOLUTION AS\n",
    "SELECT * FROM edl_hc_datamart.gold.fact_job_history WHERE dept_key = 'ADM';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53b7c8d5-7a02-4121-8986-4e2aaf6bd22c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salary Aggregation by Department and Job Title"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "\"\"\"\n",
    "Aggregates salary statistics by department and job title.\n",
    "\n",
    "Steps:\n",
    "1. Loads job and employee tables from Silver layer.\n",
    "2. Joins jobs with employees to associate department codes.\n",
    "3. Groups by department and job title, calculating total, average, minimum, and maximum salary.\n",
    "4. Writes the aggregated results to the Gold layer as 'salary_agg_by_dept_job' Delta table.\n",
    "\"\"\"\n",
    "\n",
    "# Load jobs and employees tables and Join jobs with employees to get dept_code\n",
    "jobs = spark.sql(\"SELECT employee_id, position_id, job_title, salary_amount FROM edl_hc_datamart.silver.jobs\")\n",
    "employees = spark.sql(\"SELECT employee_id, dept_code FROM edl_hc_datamart.silver.employees\")\n",
    "jobs_with_dept = jobs.join(employees, jobs[\"employee_id\"] == employees[\"employee_id\"], \"left\")\n",
    "\n",
    "# Aggregate salary by department and job title\n",
    "salary_agg = (jobs_with_dept\n",
    "    .groupBy(\"dept_code\", \"job_title\")\n",
    "    .agg(\n",
    "        sum(\"salary_amount\").alias(\"total_salary\"),\n",
    "        avg(\"salary_amount\").alias(\"avg_salary\"),\n",
    "        min(\"salary_amount\").alias(\"min_salary\"),\n",
    "        max(\"salary_amount\").alias(\"max_salary\")\n",
    "    )\n",
    "    .orderBy(\"dept_code\", \"job_title\")\n",
    ")\n",
    "\n",
    "salary_agg.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.gold.salary_agg_by_dept_job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e43fa0-8191-401c-a141-de9b543eeaa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW edl_hc_datamart.gold_views.salary_agg_by_dept_job_vw WITH SCHEMA EVOLUTION AS\n",
    "SELECT * FROM edl_hc_datamart.gold.salary_agg_by_dept_job WHERE dept_code = 'ADM';"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8973979244489501,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "hc_gold_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
