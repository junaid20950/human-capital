{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeaa46a8-cd5d-4778-b92b-4d847ad40a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.text(\"load_date\", \"\")\n",
    "load_date = dbutils.widgets.get(\"load_date\")\n",
    "print(f\"SILVER load_date = {load_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f979962-7e0d-44d5-9357-198cc758bfe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, trim, regexp_replace, when, to_date, date_format, expr\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41e26df-539b-40d6-b984-74b608081402",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Employee transformation with Incremental load"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleans and normalizes bronze Delta tables for employees, departments, and jobs,\n",
    "then writes the processed data to silver Delta tables in the specified Volumes paths.\n",
    "\n",
    "- Employees: Trims and sanitizes names, fills missing emails, normalizes date_of_birth.\n",
    "- Departments: Trims and sanitizes department names.\n",
    "- Jobs: Trims and sanitizes job titles, normalizes start/end dates.\n",
    "\n",
    "Output: Silver Delta tables for employees, departments, and jobs.\n",
    "\"\"\"\n",
    "\n",
    "df_emp_sl = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "      employee_id,\n",
    "      first_name,\n",
    "      last_name,\n",
    "      date_of_birth,\n",
    "      dept_code,\n",
    "      email\n",
    "    FROM edl_hc_datamart.bronze.employees\n",
    "    WHERE _meta_load_date = '{load_date}'\n",
    "    \"\"\"\n",
    ").withColumn(\"first_name\", regexp_replace(trim(col(\"first_name\")), r\"[^A-Za-z0-9 ]\", \"_\")\n",
    ").withColumn(\"last_name\",  regexp_replace(trim(col(\"last_name\")),  r\"[^A-Za-z0-9 ]\", \"_\")\n",
    ").withColumn(\"email\", when(col(\"email\").isNull() | (trim(col(\"email\"))==\"\"), \"unknown@example.com\")\n",
    "                     .otherwise(trim(col(\"email\")))\n",
    ").withColumn(\"date_of_birth\", to_date(col(\"date_of_birth\"))\n",
    ").withColumn(\"date_of_birth_fmt\", date_format(col(\"date_of_birth\"), \"dd-MMM-yyyy\")\n",
    ").select(\n",
    "    \"employee_id\", \"first_name\", \"last_name\",\n",
    "    \"date_of_birth\", \"date_of_birth_fmt\",\n",
    "    \"dept_code\", \"email\"\n",
    ")\n",
    "\n",
    "# Load target table\n",
    "target = DeltaTable.forName(spark, \"edl_hc_datamart.silver.employees\")\n",
    "\n",
    "# Prepare source DataFrame with SCD columns\n",
    "df_emp_scd = df_emp_sl.withColumn(\"is_active\", expr(\"true\")) \\\n",
    "                     .withColumn(\"effective_date\", current_timestamp()) \\\n",
    "                     .withColumn(\"end_date\", lit('1900-01-01').cast('timestamp'))\n",
    "# Mark old records as inactive\n",
    "target.alias(\"tgt\").merge(\n",
    "    df_emp_scd.alias(\"src\"),\n",
    "    \"tgt.employee_id = src.employee_id AND tgt.is_active = true\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"is_active\": \"false\",\n",
    "    \"end_date\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "df_emp = df_emp_scd.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.silver.employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3548b943-94c9-41a3-82e2-6af6e5f41907",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767652311673}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1767658476880}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3122af0b-bc0f-47a9-96fd-9e0008f5fecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a35300-8097-4210-b731-d5605aefbecb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Department transformation with Incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Departments\n",
    "\n",
    "df_dept_sl = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "      regexp_replace(\n",
    "          regexp_replace(trim(dept_name), '[^A-Za-z0-9 ]', '_'),\n",
    "          '_+',\n",
    "          '_'\n",
    "      ) AS dept_name,\n",
    "      -- Normalize dept_code: trim + uppercase for consistency\n",
    "      upper(trim(dept_code)) AS dept_code\n",
    "    FROM edl_hc_datamart.bronze.departments\n",
    "    WHERE _meta_load_date = '{load_date}'\n",
    "    \"\"\"\n",
    ").select(\"dept_name\", \"dept_code\")\n",
    "\n",
    "\n",
    "# Load target table\n",
    "target_dept = DeltaTable.forName(spark, \"edl_hc_datamart.silver.departments\")\n",
    "\n",
    "# Prepare source DataFrame with SCD columns\n",
    "df_dept_scd = df_dept_sl.withColumn(\"is_active\", expr(\"true\")) \\\n",
    "                        .withColumn(\"effective_date\", current_timestamp()) \\\n",
    "                        .withColumn(\"end_date\", lit('1900-01-01').cast('timestamp'))\n",
    "\n",
    "# Mark old records as inactive\n",
    "target_dept.alias(\"tgt\").merge(\n",
    "    df_dept_scd.alias(\"src\"),\n",
    "    \"tgt.dept_code = src.dept_code AND tgt.is_active = true\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"is_active\": \"false\",\n",
    "    \"end_date\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "df_dept_scd.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.silver.departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc919e28-6a86-4178-92ac-5e1252f6732e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Jobs Silver Table with Compensation Extraction"
    }
   },
   "outputs": [],
   "source": [
    "# Jobs\n",
    "df_jobs_sl = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "      employee_id,\n",
    "      end_date,\n",
    "      job_title,\n",
    "      position_id,\n",
    "      start_date,\n",
    "      status,\n",
    "      salary_amount,\n",
    "      salary_currency,\n",
    "      salary_frequency,\n",
    "      salary_effective_from,\n",
    "      salary_effective_to\n",
    "    FROM edl_hc_datamart.bronze.jobs\n",
    "    WHERE _meta_load_date = '{load_date}'\n",
    "    \"\"\"\n",
    ").withColumn(\"job_title\",\n",
    "    regexp_replace(trim(col(\"job_title\")), r\"[^A-Za-z0-9 ]\", \"_\")\n",
    ").withColumn(\"start_date\", to_date(col(\"start_date\")))\\\n",
    ".withColumn(\"end_date\",   to_date(col(\"end_date\")))\\\n",
    ".withColumn(\"start_date_fmt\", date_format(col(\"start_date\"), \"dd-MMM-yyyy\"))\\\n",
    ".withColumn(\"end_date_fmt\",\n",
    "    when(col(\"end_date\").isNotNull(), date_format(col(\"end_date\"), \"dd-MMM-yyyy\"))\n",
    ").withColumn(\"salary_currency\", upper(trim(col(\"salary_currency\")))).withColumn(\"salary_frequency\", upper(trim(col(\"salary_frequency\")))).select(\n",
    "    \"employee_id\",\n",
    "    \"position_id\",\n",
    "    \"job_title\",\n",
    "    \"status\",\n",
    "    \"start_date\", \"start_date_fmt\",\n",
    "    \"end_date\",   \"end_date_fmt\",\n",
    "    \"salary_amount\",\n",
    "    \"salary_currency\",\n",
    "    \"salary_frequency\",\n",
    "    \"salary_effective_from\",\n",
    "    \"salary_effective_to\"\n",
    ")\n",
    "\n",
    "# Load target table\n",
    "target_jobs = DeltaTable.forName(spark, \"edl_hc_datamart.silver.jobs\")\n",
    "\n",
    "df_jobs_scd = df_jobs_sl.withColumn(\"employee_id\", col(\"employee_id\").cast(\"int\")) \\\n",
    "                        .withColumn(\"salary_amount\", col(\"salary_amount\").cast(\"int\")) \\\n",
    "                        .withColumn(\"is_active\", expr(\"true\")) \\\n",
    "                        .withColumn(\"effective_date\", current_timestamp()) \\\n",
    "                        .withColumn(\"effective_end_date\", lit('1900-01-01').cast('timestamp'))\n",
    "\n",
    "# Mark old records as inactive and new records as active during insert\n",
    "target_jobs.alias(\"tgt\").merge(\n",
    "    df_jobs_scd.alias(\"src\"),\n",
    "    \"tgt.employee_id = src.employee_id AND tgt.position_id = src.position_id AND tgt.is_active = true\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"is_active\": \"false\",\n",
    "    \"effective_end_date\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "df_jobs_scd.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.silver.jobs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45260096-d927-449d-8289-14c978503983",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Attendance transformation with Incremental Load"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleans and normalizes the bronze Delta attendance table, then writes the processed data to the silver Delta table with SCD logic.\n",
    "\n",
    "- Renames columns for consistency.\n",
    "- Trims and sanitizes attendance_date.\n",
    "- Normalizes attendance_date to date type and formats as dd-MMM-yyyy.\n",
    "- Standardizes present to boolean, handling various representations.\n",
    "- Fills missing present values as False (absent).\n",
    "- Adds SCD columns: is_active, effective_date, end_date.\n",
    "\n",
    "Output: Silver Delta table for attendance.\n",
    "\"\"\"\n",
    "\n",
    "# Data cleaning and transformation\n",
    "\n",
    "df_att_sl = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "      employee_id,\n",
    "      present,\n",
    "      date\n",
    "    FROM edl_hc_datamart.bronze.attendance\n",
    "    WHERE _meta_load_date = '{load_date}'\n",
    "    \"\"\"\n",
    ").withColumn(\"attendance_date\", to_date(regexp_replace(col(\"date\"), r\"[^0-9\\-]\", \"\")))\\\n",
    ".withColumn(\"attendance_date_fmt\", date_format(col(\"attendance_date\"), \"dd-MMM-yyyy\"))\\\n",
    ".drop(\"date\")\\\n",
    ".withColumn(\n",
    "    \"present\",\n",
    "    when(col(\"present\").cast(\"string\").isin(\"TRUE\",\"True\",\"true\",\"1\",\"Y\"), True)\n",
    "    .when(col(\"present\").cast(\"string\").isin(\"FALSE\",\"False\",\"false\",\"0\",\"N\"), False)\n",
    "    .otherwise(expr(\"try_cast(present as boolean)\"))\n",
    ")\\\n",
    ".withColumn(\"present\", when(col(\"present\").isNull(), False).otherwise(col(\"present\")))\\\n",
    ".select(\"employee_id\", \"attendance_date\", \"attendance_date_fmt\", \"present\")\n",
    "\n",
    "\n",
    "# Load target table\n",
    "target_att = DeltaTable.forName(spark, \"edl_hc_datamart.silver.attendance\")\n",
    "\n",
    "# Prepare source DataFrame with SCD columns\n",
    "df_att_scd = df_att_sl.withColumn(\"is_active\", expr(\"true\")) \\\n",
    "                      .withColumn(\"effective_date\", current_timestamp()) \\\n",
    "                      .withColumn(\"end_date\", lit('1900-01-01').cast('timestamp'))\n",
    "\n",
    "# Mark old records as inactive\n",
    "target_att.alias(\"tgt\").merge(\n",
    "    df_att_scd.alias(\"src\"),\n",
    "    \"tgt.employee_id = src.employee_id AND tgt.attendance_date = src.attendance_date AND tgt.is_active = true\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"is_active\": \"false\",\n",
    "    \"end_date\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "df_att_scd.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"edl_hc_datamart.silver.attendance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c204dc-e51e-4d82-91a3-dbdfed32b3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_att_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecabfa8-2609-4ba7-816e-56fa498074b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/FileStore\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5183317486039935,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "hc_silver_notebook",
   "widgets": {
    "load_date": {
     "currentValue": "2026-02-14",
     "nuid": "2a6ea9ae-390d-4e39-9435-3972e0a33422",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "load_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "load_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
