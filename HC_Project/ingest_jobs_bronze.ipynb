{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e580bd7-263c-4863-b39d-ef6e1d0f9140",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "last_load_date_row = spark.sql(\"\"\"\n",
    "       SELECT MAX(last_load_date) AS last_load_date --suppose we are taking business_date\n",
    "       FROM edl_hc_datamart.audit.audit_ingestion\n",
    "       WHERE pipeline_name = 'bronze_ingest_pipeline' AND last_status = 'Success'\n",
    "   \"\"\").collect()\n",
    "\n",
    "last_load_date = last_load_date_row[0]['last_load_date'] if last_load_date_row and last_load_date_row[0]['last_load_date'] else None\n",
    "formatted_date = None  # Minimal fix: ensure variable is always defined\n",
    "# print(last_load_date)\n",
    "if last_load_date:\n",
    "    formatted_date = last_load_date.strftime(\"%Y-%m-%d\")\n",
    "    display(formatted_date)\n",
    "else:\n",
    "    formatted_date = \"1900-01-01\"\n",
    "    display(formatted_date)\n",
    "\n",
    "dbutils.widgets.text(\"load_date\", formatted_date)\n",
    "load_date = dbutils.widgets.get(\"load_date\")\n",
    "displayHTML(load_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1750a0-403b-44de-bc5d-08646c55a744",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "#begin\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import json, ast\n",
    "\n",
    "# --- ADD: imports for date handling ---\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "CATALOG  = \"edl_hc_datamart\"\n",
    "SCHEMA   = \"bronze\"\n",
    "META_CSV = \"/Volumes/edl_hc_datamart/config/metadata_config/ingestion_metadata.csv\"\n",
    "\n",
    "# --- ADD: date parsing and parameter retrieval ---\n",
    "def parse_load_date(value: str) -> str:\n",
    "    \"\"\"Validate yyyy-MM-dd and return normalized string.\"\"\"\n",
    "    if not value:\n",
    "        raise ValueError(\"load_date is empty.\")\n",
    "    if not re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", value):\n",
    "        raise ValueError(f\"Invalid load_date '{value}'. Expected format yyyy-MM-dd.\")\n",
    "    datetime.strptime(value, \"%Y-%m-%d\")\n",
    "    return value\n",
    "\n",
    "    # 2) Widgets\n",
    "    try:\n",
    "        dbutils.widgets.text(\"load_date\", \"\")\n",
    "        ld = dbutils.widgets.get(\"load_date\")\n",
    "        if ld:\n",
    "            return parse_load_date(ld)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] widgets unavailable: {e}\")\n",
    "\n",
    "    # 3) Default\n",
    "    today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    print(f\"[INFO] defaulting load_date to {today} (UTC)\")\n",
    "    return today\n",
    "\n",
    "# --- ADD: load_date value early for logging & reuse ---\n",
    "# LOAD_DATE = get_load_date()\n",
    "LOAD_DATE = load_date\n",
    "print(f\"[BRONZE] Using load_date: {LOAD_DATE}\")\n",
    "\n",
    "def trim_columns(df):\n",
    "    return df.select([F.col(c).alias(c.strip()) for c in df.columns])\n",
    "\n",
    "def parse_options(raw, fmt, landed_path):\n",
    "    fmt = (fmt or \"csv\").strip().lower()\n",
    "    if not raw or raw.strip() == \"\":\n",
    "        raw = \"\"\n",
    "    s = (raw.strip()\n",
    "         .replace('“','\"').replace('”','\"')\n",
    "         .replace(\"‘\",\"'\").replace(\"’\",\"'\"))\n",
    "    # 1) JSON\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except: pass\n",
    "    # 2) Python literal dict\n",
    "    try:\n",
    "        lit = ast.literal_eval(s)\n",
    "        if isinstance(lit, dict):\n",
    "            return lit\n",
    "    except: pass\n",
    "    # 3) key=value;key=value\n",
    "    try:\n",
    "        opts = {}\n",
    "        for part in s.split(';'):\n",
    "            if '=' in part:\n",
    "                k, v = part.split('=', 1)\n",
    "                k = k.strip(); v = v.strip()\n",
    "                if v.lower() in ('true','false'):\n",
    "                    opts[k] = (v.lower() == 'true')\n",
    "                else:\n",
    "                    opts[k] = v.strip('\"').strip(\"'\")\n",
    "        if opts: return opts\n",
    "    except: pass\n",
    "    # 4) defaults\n",
    "    if fmt == \"csv\":\n",
    "        if landed_path and landed_path.lower().endswith(\".tsv\"):\n",
    "            return {\"header\": True, \"sep\": \"\\t\", \"inferSchema\": True}\n",
    "        return {\"header\": True, \"inferSchema\": True}\n",
    "    if fmt == \"json\":\n",
    "        return {\"multiline\": True}\n",
    "    if fmt == \"tsv\":\n",
    "        return {\"header\": True, \"sep\": \"\\t\", \"inferSchema\": True}\n",
    "    return {}\n",
    "\n",
    "def source_file_col(df, landed_path: str):\n",
    "    if \"_metadata\" in df.columns:\n",
    "        try:\n",
    "            _ = df.select(F.col(\"_metadata.file_path\")).limit(1).collect()\n",
    "            return F.col(\"_metadata.file_path\")\n",
    "        except: pass\n",
    "    return F.lit(landed_path)\n",
    "\n",
    "def ingestion_ts_col(ing_str: str):\n",
    "    norm = F.regexp_replace(F.lit(ing_str or \"\"), r\"T\", \" \")\n",
    "    ts   = F.try_to_timestamp(norm)\n",
    "    return F.coalesce(ts, F.current_timestamp())\n",
    "\n",
    "# -----------------------------------------\n",
    "# UC catalog/schema\n",
    "# -----------------------------------------\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE {SCHEMA}\")\n",
    "\n",
    "# ------------------------\n",
    "# Read metadata\n",
    "# ------------------------\n",
    "meta_df = trim_columns(spark.read.option(\"header\", True).csv(META_CSV))\n",
    "\n",
    "# ---------------------------\n",
    "# Loop metadata rows\n",
    "# ---------------------------\n",
    "for m in meta_df.collect():\n",
    "    md = {k: m[k] for k in meta_df.columns}\n",
    "\n",
    "    source_name  = md.get('source_name')\n",
    "    landed_path  = md.get('landed_path')\n",
    "    bronze_table = md.get('bronze_table')\n",
    "    source_type  = (md.get('source_type') or \"file\").lower()\n",
    "    fmt          = (md.get('format') or 'csv').lower()\n",
    "    options      = parse_options(md.get('options'), fmt, landed_path)\n",
    "\n",
    "    if not source_name or not bronze_table:\n",
    "        print(f\"Skipping row: missing source_name or table {md}\")\n",
    "        continue\n",
    "\n",
    "    full_table = f\"{CATALOG}.{bronze_table}\"\n",
    "    print(f\"\\nProcessing {source_name} (type={source_type}) -> {full_table}\")\n",
    "\n",
    "    # Only process file sources in this run\n",
    "    if source_type != \"file\":\n",
    "        print(f\"Skipping non-file source: {source_type} ({source_name})\")\n",
    "        continue\n",
    "\n",
    "    if not landed_path:\n",
    "        print(f\"Skipping row: missing landed_path for file source {md}\")\n",
    "        continue\n",
    "\n",
    "    # ----- OPTIONAL: if your landed paths are partitioned by date, e.g. /raw/hr/YYYY-MM-DD -----\n",
    "    # If so, you can derive a dated path:\n",
    "    # landed_path = f\"{landed_path.rstrip('/')}/{LOAD_DATE}\"\n",
    "\n",
    "    # ----- read dataset -----\n",
    "    reader = spark.read\n",
    "    for k, v in options.items():\n",
    "        reader = reader.option(k, v)\n",
    "    df = reader.format(fmt).load(landed_path)\n",
    "\n",
    "    # 3. Filter for incremental rows\n",
    "    if md.get('pipeline_name') == 'bronze_ingest_pipeline' and last_load_date:\n",
    "        df = df.filter(df['business_date'] > last_load_date)\n",
    "\n",
    "    # Print schema for visibility\n",
    "    print(\"[SCHEMA] Input DF:\")\n",
    "    # df.printSchema()\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Robust JSON parsing: from_json + schema for 'compensation'\n",
    "    # -----------------------------------------------------\n",
    "    if \"compensation\" in df.columns:\n",
    "        comp_field = next((f for f in df.schema.fields if f.name == \"compensation\"), None)\n",
    "        comp_dt = comp_field.dataType if comp_field else None\n",
    "        print(f\"[INFO] compensation datatype: {comp_dt}\")\n",
    "\n",
    "        if isinstance(comp_dt, T.StructType):\n",
    "            df = (df\n",
    "                .withColumn(\"salary_amount\",         F.col(\"compensation.salary.amount\").cast(\"double\"))\n",
    "                .withColumn(\"salary_currency\",       F.col(\"compensation.salary.currency\"))\n",
    "                .withColumn(\"salary_frequency\",      F.col(\"compensation.salary.frequency\"))\n",
    "                .withColumn(\"salary_effective_from\", F.to_date(F.col(\"compensation.salary.effective_from\")))\n",
    "                .withColumn(\"salary_effective_to\",   F.to_date(F.col(\"compensation.salary.effective_to\")))\n",
    "                .drop(\"compensation\")\n",
    "            )\n",
    "        elif isinstance(comp_dt, T.StringType):\n",
    "            comp_schema = T.StructType([\n",
    "                T.StructField(\"salary\", T.StructType([\n",
    "                    T.StructField(\"effective_to\",   T.StringType(), True),\n",
    "                    T.StructField(\"amount\",         T.DoubleType(), True),\n",
    "                    T.StructField(\"currency\",       T.StringType(), True),\n",
    "                    T.StructField(\"effective_from\", T.StringType(), True),\n",
    "                    T.StructField(\"frequency\",      T.StringType(), True),\n",
    "                ]), True)\n",
    "            ])\n",
    "            df = (df\n",
    "                .withColumn(\"compensation_struct\", F.from_json(F.col(\"compensation\"), comp_schema))\n",
    "                .withColumn(\"salary_amount\",         F.col(\"compensation_struct.salary.amount\"))\n",
    "                .withColumn(\"salary_currency\",       F.col(\"compensation_struct.salary.currency\"))\n",
    "                .withColumn(\"salary_frequency\",      F.col(\"compensation_struct.salary.frequency\"))\n",
    "                .withColumn(\"salary_effective_from\", F.to_date(F.col(\"compensation_struct.salary.effective_from\")))\n",
    "                .withColumn(\"salary_effective_to\",   F.to_date(F.col(\"compensation_struct.salary.effective_to\")))\n",
    "                .drop(\"compensation_struct\")\n",
    "                .drop(\"compensation\")\n",
    "            )\n",
    "        else:\n",
    "            print(f\"[WARN] Unexpected compensation type: {comp_dt}. Skipping flattening.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Metadata enrichment\n",
    "    # ----------------------------\n",
    "    file_path = source_file_col(df, landed_path)\n",
    "    ing_ts    = ingestion_ts_col(md.get('ingestion_ts'))\n",
    "\n",
    "    df_enriched = (df\n",
    "        .withColumn(\"_meta_pipeline_name\",  F.lit(md.get('pipeline_name')))\n",
    "        .withColumn(\"_meta_source_name\",    F.lit(source_name))\n",
    "        .withColumn(\"_meta_batch_id\",       F.lit(md.get('batch_id')))\n",
    "        .withColumn(\"_meta_run_id\",         F.lit(md.get('run_id')))\n",
    "        .withColumn(\"_meta_schema_version\", F.lit(md.get('schema_version')))\n",
    "        .withColumn(\"_meta_producer_system\",F.lit(md.get('producer_system')))\n",
    "        .withColumn(\"_meta_ingestion_user\", F.lit(md.get('ingestion_user')))\n",
    "        .withColumn(\"_meta_ingestion_ts\",   ing_ts)\n",
    "        .withColumn(\"_meta_source_file\",    file_path)\n",
    "        .withColumn(\"_meta_load_time\",      F.current_timestamp())\n",
    "        .withColumn(\"_meta_load_date\",      F.lit(LOAD_DATE))   # <-- ADDED\n",
    "    ).drop(\"compensation\")\n",
    "    # df_enriched.printSchema()\n",
    "    # display(df_enriched)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Write to UC table (overwrite as per your original)\n",
    "    # ----------------------------\n",
    "    (df_enriched.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(full_table)\n",
    "    )\n",
    "\n",
    "    print(f\"Written to {full_table} for load_date={LOAD_DATE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3b03b9-5b1a-4783-8792-dc3f7f317730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "last_load_date_row = spark.sql(\"\"\"\n",
    "       SELECT MAX(business_date) AS load_date\n",
    "       FROM edl_hc_datamart.bronze.employees\n",
    "   \"\"\")\n",
    "# display(last_load_date_row)\n",
    "# Extract the value from the DataFrame\n",
    "last_load_date = last_load_date_row.collect()[0]['load_date'] if last_load_date_row.count() > 0 else None\n",
    "# Format as string (YYYY-MM-DD)\n",
    "load_date = last_load_date.strftime(\"%Y-%m-%d\") if last_load_date else \"\"\n",
    "\n",
    "display(load_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e0379b-a5e1-4f8e-93b5-828b90d568bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Audit ingestion update logic (fix attempt field)"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timezone\n",
    "\n",
    "AUDIT_TABLE = \"edl_hc_datamart.audit.audit_ingestion\"\n",
    "\n",
    "def update_audit_ingestion(md, records_read, records_written, status, columns_loaded, layer, error_message=None):\n",
    "    now = datetime.now(timezone.utc)\n",
    "    attempt_val = md.get('attempt')\n",
    "    if attempt_val is None or attempt_val == \"\":\n",
    "        attempt_val = 1\n",
    "    else:\n",
    "        try:\n",
    "            attempt_val = int(attempt_val)\n",
    "        except Exception:\n",
    "            attempt_val = 1\n",
    "    audit_dict = {\n",
    "        \"pipeline_name\":            str(md.get('pipeline_name') or \"\"),\n",
    "        \"source_type\":              str(md.get('source_type') or \"file\"),\n",
    "        \"source_name\":              str(md.get('source_name') or \"\"),\n",
    "        \"bronze_table\":             str(md.get('bronze_table') or \"\"),\n",
    "        \"batch_id\":                 str(md.get('batch_id') or \"\"),\n",
    "        \"run_id\":                   str(md.get('run_id') or \"\"),\n",
    "        \"trigger_type\":             str(md.get('trigger_type') or \"\"),\n",
    "        \"attempt\":                  attempt_val,\n",
    "        \"run_start_ts\":             now,\n",
    "        \"run_end_ts\":               now,\n",
    "        \"duration_ms\":              int(0),\n",
    "        \"last_status\":              str(status),\n",
    "        \"records_read\":             int(records_read),\n",
    "        \"records_written\":          int(records_written),\n",
    "        \"error_count\":              int(1 if error_message else 0),\n",
    "        \"error_message\":            str(error_message or \"\"),\n",
    "        \"watermark_col\":            str(md.get('watermark_col') or \"\"),\n",
    "        \"last_success_watermark_value\": str(md.get('last_success_watermark_value') or \"\"),\n",
    "        \"current_run_high_watermark_value\": str(md.get('current_run_high_watermark_value') or \"\"),\n",
    "        \"file_checkpoint_path\":     str(md.get('file_checkpoint_path') or \"\"),\n",
    "        \"schema_version_applied\":   str(md.get('schema_version') or \"\"),\n",
    "        \"producer_system\":          str(md.get('producer_system') or \"\"),\n",
    "        \"ingestion_user\":           str(md.get('ingestion_user') or \"\"),\n",
    "        \"notes\":                    str(md.get('notes') or \"\"),\n",
    "        \"last_load_date\":           date.fromisoformat(load_date),  # Minimal fix\n",
    "        \"created_at\":               now,\n",
    "        \"updated_at\":               now\n",
    "    }\n",
    "    audit_df = spark.createDataFrame([audit_dict])\n",
    "    # display(audit_df)\n",
    "    audit_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").saveAsTable(AUDIT_TABLE)\n",
    "    # display(audit_df)\n",
    "\n",
    "# --- Integrate audit update in main loop ---\n",
    "for m in meta_df.collect():\n",
    "    md = {k: m[k] for k in meta_df.columns}\n",
    "    source_name  = md.get('source_name')\n",
    "    landed_path  = md.get('landed_path')\n",
    "    bronze_table = md.get('bronze_table')\n",
    "    source_type  = (md.get('source_type') or \"file\").lower()\n",
    "    fmt          = (md.get('format') or 'csv').lower()\n",
    "    options      = parse_options(md.get('options'), fmt, landed_path)\n",
    "\n",
    "    # if not source_name or not bronze_table:\n",
    "    #     print(f\"Skipping row: missing source_name or table {md}\")\n",
    "    #     continue\n",
    "\n",
    "    full_table = f\"{CATALOG}.{bronze_table}\"\n",
    "    # print(f\"\\nProcessing {source_name} (type={source_type}) -> {full_table}\")\n",
    "\n",
    "    # if source_type != \"file\":\n",
    "    #     print(f\"Skipping non-file source: {source_type} ({source_name})\")\n",
    "    #     continue\n",
    "\n",
    "    # if not landed_path:\n",
    "    #     print(f\"Skipping row: missing landed_path for file source {md}\")\n",
    "    #     continue\n",
    "\n",
    "    try:\n",
    "        reader = spark.read\n",
    "        for k, v in options.items():\n",
    "            reader = reader.option(k, v)\n",
    "        df = reader.format(fmt).load(landed_path)\n",
    "        records_read = df.count()\n",
    "        # display(df)\n",
    "\n",
    "        # Insert records for all bronze tables (employees, departments, etc.)\n",
    "        (df.write\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .format(\"delta\")\n",
    "            .saveAsTable(full_table)\n",
    "        )\n",
    "        records_written = df.count()\n",
    "        columns_loaded = \",\".join(df.columns)\n",
    "        layer = \"bronze\"\n",
    "        # display(df)\n",
    "        update_audit_ingestion(md, records_read, records_written, \"Success\", columns_loaded, layer)\n",
    "        # print(f\"Written to {full_table} for load_date={LOAD_DATE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        columns_loaded = \"\"\n",
    "        layer = \"bronze\"\n",
    "        update_audit_ingestion(md, 0, 0, \"Failure\", columns_loaded, layer, error_message=str(e))\n",
    "        print(f\"[ERROR] Failed to load {source_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8850844335562790,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_jobs_bronze",
   "widgets": {
    "load_date": {
     "currentValue": "",
     "nuid": "b7d7ddb3-b803-424a-a355-1cc845a2412e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1900-01-01",
      "label": null,
      "name": "load_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1900-01-01",
      "label": null,
      "name": "load_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
