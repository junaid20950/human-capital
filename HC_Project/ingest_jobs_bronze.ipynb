{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb03b92f-427f-4f10-b7bf-0408c0ef133b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"load_date\", \"\")\n",
    "load_date = dbutils.widgets.get(\"load_date\")\n",
    "display(load_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1750a0-403b-44de-bc5d-08646c55a744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import json, ast\n",
    "\n",
    "# --- ADD: imports for date handling ---\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "CATALOG  = \"edl_hc_mart\"\n",
    "SCHEMA   = \"bronze\"\n",
    "META_CSV = \"/Volumes/edl_hc_mart/config/metadata_config/updated_ingestion_metadata.csv\"\n",
    "\n",
    "# --- ADD: date parsing and parameter retrieval ---\n",
    "def parse_load_date(value: str) -> str:\n",
    "    \"\"\"Validate yyyy-MM-dd and return normalized string.\"\"\"\n",
    "    if not value:\n",
    "        raise ValueError(\"load_date is empty.\")\n",
    "    if not re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", value):\n",
    "        raise ValueError(f\"Invalid load_date '{value}'. Expected format yyyy-MM-dd.\")\n",
    "    datetime.strptime(value, \"%Y-%m-%d\")\n",
    "    return value\n",
    "\n",
    "def get_load_date():\n",
    "    \"\"\"\n",
    "    Try CLI arg --load_date first; fallback to Databricks widgets; default to UTC today.\n",
    "    Works both in Community Edition notebooks and external Python runners.\n",
    "    \"\"\"\n",
    "    # 1) CLI\n",
    "    try:\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description=\"Bronze ingestion with load_date\")\n",
    "        parser.add_argument(\"--load_date\", required=False, help=\"yyyy-MM-dd\")\n",
    "        args, _ = parser.parse_known_args()\n",
    "        if args.load_date:\n",
    "            return parse_load_date(args.load_date)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] argparse failed or not provided: {e}\")\n",
    "\n",
    "    # 2) Widgets\n",
    "    try:\n",
    "        dbutils.widgets.text(\"load_date\", \"\")\n",
    "        ld = dbutils.widgets.get(\"load_date\")\n",
    "        if ld:\n",
    "            return parse_load_date(ld)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] widgets unavailable: {e}\")\n",
    "\n",
    "    # 3) Default\n",
    "    today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    print(f\"[INFO] defaulting load_date to {today} (UTC)\")\n",
    "    return today\n",
    "\n",
    "# --- ADD: load_date value early for logging & reuse ---\n",
    "LOAD_DATE = get_load_date()\n",
    "print(f\"[BRONZE] Using load_date: {LOAD_DATE}\")\n",
    "\n",
    "def trim_columns(df):\n",
    "    return df.select([F.col(c).alias(c.strip()) for c in df.columns])\n",
    "\n",
    "def parse_options(raw, fmt, landed_path):\n",
    "    fmt = (fmt or \"csv\").strip().lower()\n",
    "    if not raw or raw.strip() == \"\":\n",
    "        raw = \"\"\n",
    "    s = (raw.strip()\n",
    "         .replace('“','\"').replace('”','\"')\n",
    "         .replace(\"‘\",\"'\").replace(\"’\",\"'\"))\n",
    "    # 1) JSON\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except: pass\n",
    "    # 2) Python literal dict\n",
    "    try:\n",
    "        lit = ast.literal_eval(s)\n",
    "        if isinstance(lit, dict):\n",
    "            return lit\n",
    "    except: pass\n",
    "    # 3) key=value;key=value\n",
    "    try:\n",
    "        opts = {}\n",
    "        for part in s.split(';'):\n",
    "            if '=' in part:\n",
    "                k, v = part.split('=', 1)\n",
    "                k = k.strip(); v = v.strip()\n",
    "                if v.lower() in ('true','false'):\n",
    "                    opts[k] = (v.lower() == 'true')\n",
    "                else:\n",
    "                    opts[k] = v.strip('\"').strip(\"'\")\n",
    "        if opts: return opts\n",
    "    except: pass\n",
    "    # 4) defaults\n",
    "    if fmt == \"csv\":\n",
    "        if landed_path and landed_path.lower().endswith(\".tsv\"):\n",
    "            return {\"header\": True, \"sep\": \"\\t\", \"inferSchema\": True}\n",
    "        return {\"header\": True, \"inferSchema\": True}\n",
    "    if fmt == \"json\":\n",
    "        return {\"multiline\": True}\n",
    "    if fmt == \"tsv\":\n",
    "        return {\"header\": True, \"sep\": \"\\t\", \"inferSchema\": True}\n",
    "    return {}\n",
    "\n",
    "def source_file_col(df, landed_path: str):\n",
    "    if \"_metadata\" in df.columns:\n",
    "        try:\n",
    "            _ = df.select(F.col(\"_metadata.file_path\")).limit(1).collect()\n",
    "            return F.col(\"_metadata.file_path\")\n",
    "        except: pass\n",
    "    return F.lit(landed_path)\n",
    "\n",
    "def ingestion_ts_col(ing_str: str):\n",
    "    norm = F.regexp_replace(F.lit(ing_str or \"\"), r\"T\", \" \")\n",
    "    ts   = F.try_to_timestamp(norm)\n",
    "    return F.coalesce(ts, F.current_timestamp())\n",
    "\n",
    "# -----------------------------------------\n",
    "# UC catalog/schema\n",
    "# -----------------------------------------\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE {SCHEMA}\")\n",
    "\n",
    "# ------------------------\n",
    "# Read metadata\n",
    "# ------------------------\n",
    "meta_df = trim_columns(spark.read.option(\"header\", True).csv(META_CSV))\n",
    "\n",
    "# ---------------------------\n",
    "# Loop metadata rows\n",
    "# ---------------------------\n",
    "for m in meta_df.collect():\n",
    "    md = {k: m[k] for k in meta_df.columns}\n",
    "\n",
    "    source_name  = md.get('source_name')\n",
    "    landed_path  = md.get('landed_path')\n",
    "    bronze_table = md.get('bronze_table')\n",
    "    source_type  = (md.get('source_type') or \"file\").lower()\n",
    "    fmt          = (md.get('format') or 'csv').lower()\n",
    "    options      = parse_options(md.get('options'), fmt, landed_path)\n",
    "\n",
    "    if not source_name or not bronze_table:\n",
    "        print(f\"Skipping row: missing source_name or table {md}\")\n",
    "        continue\n",
    "\n",
    "    full_table = f\"{CATALOG}.{bronze_table}\"\n",
    "    print(f\"\\nProcessing {source_name} (type={source_type}) -> {full_table}\")\n",
    "\n",
    "    # Only process file sources in this run\n",
    "    if source_type != \"file\":\n",
    "        print(f\"Skipping non-file source: {source_type} ({source_name})\")\n",
    "        continue\n",
    "\n",
    "    if not landed_path:\n",
    "        print(f\"Skipping row: missing landed_path for file source {md}\")\n",
    "        continue\n",
    "\n",
    "    # ----- OPTIONAL: if your landed paths are partitioned by date, e.g. /raw/hr/YYYY-MM-DD -----\n",
    "    # If so, you can derive a dated path:\n",
    "    # landed_path = f\"{landed_path.rstrip('/')}/{LOAD_DATE}\"\n",
    "\n",
    "    # ----- read dataset -----\n",
    "    reader = spark.read\n",
    "    for k, v in options.items():\n",
    "        reader = reader.option(k, v)\n",
    "    df = reader.format(fmt).load(landed_path)\n",
    "\n",
    "    # Print schema for visibility\n",
    "    print(\"[SCHEMA] Input DF:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Robust JSON parsing: from_json + schema for 'compensation'\n",
    "    # -----------------------------------------------------\n",
    "    if \"compensation\" in df.columns:\n",
    "        comp_field = next((f for f in df.schema.fields if f.name == \"compensation\"), None)\n",
    "        comp_dt = comp_field.dataType if comp_field else None\n",
    "        print(f\"[INFO] compensation datatype: {comp_dt}\")\n",
    "\n",
    "        if isinstance(comp_dt, T.StructType):\n",
    "            df = (df\n",
    "                .withColumn(\"salary_amount\",         F.col(\"compensation.salary.amount\").cast(\"double\"))\n",
    "                .withColumn(\"salary_currency\",       F.col(\"compensation.salary.currency\"))\n",
    "                .withColumn(\"salary_frequency\",      F.col(\"compensation.salary.frequency\"))\n",
    "                .withColumn(\"salary_effective_from\", F.to_date(F.col(\"compensation.salary.effective_from\")))\n",
    "                .withColumn(\"salary_effective_to\",   F.to_date(F.col(\"compensation.salary.effective_to\")))\n",
    "                .drop(\"compensation\")\n",
    "            )\n",
    "        elif isinstance(comp_dt, T.StringType):\n",
    "            comp_schema = T.StructType([\n",
    "                T.StructField(\"salary\", T.StructType([\n",
    "                    T.StructField(\"effective_to\",   T.StringType(), True),\n",
    "                    T.StructField(\"amount\",         T.DoubleType(), True),\n",
    "                    T.StructField(\"currency\",       T.StringType(), True),\n",
    "                    T.StructField(\"effective_from\", T.StringType(), True),\n",
    "                    T.StructField(\"frequency\",      T.StringType(), True),\n",
    "                ]), True)\n",
    "            ])\n",
    "            df = (df\n",
    "                .withColumn(\"compensation_struct\", F.from_json(F.col(\"compensation\"), comp_schema))\n",
    "                .withColumn(\"salary_amount\",         F.col(\"compensation_struct.salary.amount\"))\n",
    "                .withColumn(\"salary_currency\",       F.col(\"compensation_struct.salary.currency\"))\n",
    "                .withColumn(\"salary_frequency\",      F.col(\"compensation_struct.salary.frequency\"))\n",
    "                .withColumn(\"salary_effective_from\", F.to_date(F.col(\"compensation_struct.salary.effective_from\")))\n",
    "                .withColumn(\"salary_effective_to\",   F.to_date(F.col(\"compensation_struct.salary.effective_to\")))\n",
    "                .drop(\"compensation_struct\")\n",
    "                .drop(\"compensation\")\n",
    "            )\n",
    "        else:\n",
    "            print(f\"[WARN] Unexpected compensation type: {comp_dt}. Skipping flattening.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Metadata enrichment\n",
    "    # ----------------------------\n",
    "    file_path = source_file_col(df, landed_path)\n",
    "    ing_ts    = ingestion_ts_col(md.get('ingestion_ts'))\n",
    "\n",
    "    df_enriched = (df\n",
    "        .withColumn(\"_meta_pipeline_name\",  F.lit(md.get('pipeline_name')))\n",
    "        .withColumn(\"_meta_source_name\",    F.lit(source_name))\n",
    "        .withColumn(\"_meta_batch_id\",       F.lit(md.get('batch_id')))\n",
    "        .withColumn(\"_meta_run_id\",         F.lit(md.get('run_id')))\n",
    "        .withColumn(\"_meta_schema_version\", F.lit(md.get('schema_version')))\n",
    "        .withColumn(\"_meta_producer_system\",F.lit(md.get('producer_system')))\n",
    "        .withColumn(\"_meta_ingestion_user\", F.lit(md.get('ingestion_user')))\n",
    "        .withColumn(\"_meta_ingestion_ts\",   ing_ts)\n",
    "        .withColumn(\"_meta_source_file\",    file_path)\n",
    "        .withColumn(\"_meta_load_time\",      F.current_timestamp())\n",
    "        .withColumn(\"_meta_load_date\",      F.lit(LOAD_DATE))   # <-- ADDED\n",
    "    ).drop(\"compensation\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Write to UC table (overwrite as per your original)\n",
    "    # ----------------------------\n",
    "    (df_enriched.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(full_table)\n",
    "    )\n",
    "\n",
    "    print(f\"Written to {full_table} for load_date={LOAD_DATE}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_jobs_bronze",
   "widgets": {
    "load_date": {
     "currentValue": "",
     "nuid": "a0e425dd-3d37-44dd-8ff1-1b928402eefa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "load_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "load_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
