{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129b3807-5023-4eea-9c36-0d5e4015516c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating Dataframe by reading files from volume\n",
    "\n",
    "#Employees\n",
    "df_emp = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/edl_hc_mart/landing/landing/employees.csv\")\n",
    "df_emp.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/edl_hc_mart/bronze/bronze/employees\")\n",
    "\n",
    "#Departments\n",
    "df_dept = spark.read.option(\"header\",\"true\").option(\"sep\",\"\\t\").option(\"inferSchema\",\"true\").csv(\"/Volumes/edl_hc_mart/landing/landing/departments.tsv\")\n",
    "df_dept.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/edl_hc_mart/bronze/bronze/departments\")\n",
    "\n",
    "\n",
    "#Attendance\n",
    "df_att  = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/edl_hc_mart/landing/landing/attendance.csv\")\n",
    "df_att.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/edl_hc_mart/bronze/bronze/attendance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c80541-38b4-4a0c-9159-c274bd846818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the jobs.json file from the specified volume using a predefined schema,\n",
    "# then write the resulting DataFrame to the bronze layer in Delta format.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "jobs_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"position_id\", StringType(), True),\n",
    "    StructField(\"job_title\", StringType(), True),\n",
    "    StructField(\"start_date\", StringType(), True),\n",
    "    StructField(\"end_date\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_jobs = (\n",
    "    spark.read\n",
    "    .schema(jobs_schema)\n",
    "    .option(\"mode\", \"FAILFAST\")  # Ensures that corrupt records cause the read to fail\n",
    "    .json(\"/Volumes/edl_hc_mart/landing/landing/jobs.json\")\n",
    ")\n",
    "\n",
    "df_jobs.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/edl_hc_mart/bronze/bronze/jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e1b50a-0675-42ad-8332-853b1eeb599f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the jobs DataFrame to visually inspect its contents\n",
    "# display(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd41bf92-6e41-44ee-9c4e-06cfbc53c85b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Validation after loading from Landing to Bronze"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Registers the loaded DataFrames as temporary views and displays the row count for each table.\n",
    "\n",
    "This code creates temporary views for jobs, employees, departments, and attendance,\n",
    "then uses Spark SQL to compute and display the count of rows in each view.\n",
    "\"\"\"\n",
    "\n",
    "# Register DataFrames as temporary views and display row counts for each table\n",
    "\n",
    "df_jobs.createOrReplaceTempView(\"jobs\")\n",
    "df_emp.createOrReplaceTempView(\"employees\")\n",
    "df_dept.createOrReplaceTempView(\"departments\")\n",
    "df_att.createOrReplaceTempView(\"attendance\")\n",
    "\n",
    "#displayed output for count of each data sources by using sparksql\n",
    "\n",
    "display(spark.sql(\"select count(*) from employees\"))\n",
    "display(spark.sql(\"select count(*) from departments\"))\n",
    "display(spark.sql(\"select count(*) from attendance\"))\n",
    "display(spark.sql(\"select count(*) from jobs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf01bc2-b35e-4850-9d0f-8b528326113d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display the row count for the attendance DataFrame.\n",
    "\n",
    "This code computes the total number of rows in the df_att DataFrame,\n",
    "which represents the attendance records loaded from the source file.\n",
    "\"\"\"\n",
    "\n",
    "# df_jobs.count()\n",
    "# df_emp.count()\n",
    "# df_dept.count()\n",
    "# df_att.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41bc58-f475-4c6e-80be-f37f98c32f78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Json total record count validation"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open(\"/Volumes/edl_hc_mart/landing/landing/jobs.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7865136549837229,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "hc_bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
